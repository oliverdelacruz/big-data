{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# <center>Big Data &ndash; Exercises</center>\n## <center>Fall 2018 &ndash; Week 2 &ndash; ETH Zurich</center>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exercise 1: Storage devices \n\nIn this exercise, we want to understand the differences between [SSD](https://en.wikipedia.org/wiki/Solid-state_drive), [HDD](https://en.wikipedia.org/wiki/Hard_disk_drive), and [SDRAM](https://en.wikipedia.org/wiki/Synchronous_dynamic_random-access_memory) in terms of __capacity__, __speed__ and __price__. \n\n### Task 1\nFill in the table below by visiting your local online hardware store and choosing the storage device with largest capacity available but optimizing for read/write speed.\nFor instance, you can visit Digitec.ch to explore the prices on [SSDs](https://www.digitec.ch/en/s1/producttype/ssd-545?tagIds=76), [HDDs](https://www.digitec.ch/en/s1/producttype/hard-drives-36?tagIds=76), and \n[SDRAMs](https://www.digitec.ch/en/s1/producttype/memory-2?tagIds=76). \nYou are free to use any other website for filling the table. \n\n\n| Storage Device | Maximum capacity, GB | Price, CHF/GB  | Read speed, GB/s | Write speed, GB/s | Link |\n| --------------:| --------------------:| --------------:|-----------------:|------------------:|------|\n| HDD            |                      |                |                  |                   |&nbsp;|\n| SSD            |                      |                |                  |                   |&nbsp;|\n| DRAM           |                      |                |                  |                   |&nbsp;|\n\n\n### Task 2\nAnswer the following questions:\n1. What type of storage devices above is the cheapest one?\n2. What type of storage devices above is the fastest in terms of read speed?"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Solution:\n\nLooking at Digitec, we fill the table as follows:\n\n| Storage Device | Maximum capacity, GB | Price, CHF/GB  | Read speed, GB/s | Write speed, GB/s | Link |\n| --------------:| --------------------:| --------------:|-----------------:|------------------:|------|\n| HDD            |    10 TB;            |   0.033 CHF/GB |      0.25 GB/s   | 0.22 GB/s          | [link](https://www.digitec.ch/en/s1/product/seagate-ironwolf-10tb-35-nas-hard-drives-5961183) [link2](https://www.techradar.com/reviews/pc-mac/pc-components/storage/disk-drives-hdd-ssd/seagate-ironwolf-10tb-hard-drive-1329977/review) |\n| SSD            |    2 TB;             |   0.32  CHF/GB |      3.5  GB/s   |       2.5 GB/s    | [link](https://www.digitec.ch/en/s1/product/samsung-970-evo-2000gb-m2-2280-ssd-8351284)  |\n| DRAM           |    64 GB per module; |  12.5   CHF/GB |     60   GB/s    |       48 GB/s     | [link1](https://www.digitec.ch/en/s1/product/samsung-m386a8k40bm1-crc-1x-64gb-ddr4-2133-dimm-288-memory-8989701) [link2](https://www.techspot.com/news/62129-ddr3-vs-ddr4-raw-bandwidth-numbers.html)|\n\n\n1. HDDs are the cheapest storage device among mentioned devices \n2. DRAMs are the fastest storage device among mentioned devices "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exercise 2: Set up an Azure storage account\n\nIt is comprised of the following steps:\n1. Create a Locally redundant storage\n2. Learn features of blob's types: Append, Block, Page\n3. Test the Locally redundant storage by writing to it.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n### Step 1: Create a Locally redundant storage\n\n1. First, you need to create a storage account. To do that, go to \"Storage accounts\" in the left menu. Click on \"Add\" at the top of the blade and fill out the following form. Choose the \"exercise02\" resource group or create a new one, select \"Locally redundant storage (LRS)\" as replication mode, and let all other values at their defaults. <br>\nThe specific name is not important. Choose one whatever you want.\n\n2. Open the new storage account (you may need to wait a while before it has been created), go to \"Access keys\" (under \"Settings\") and copy one of its keys to the clipboard.\n![Image of Account](https://ethbigdata2017.blob.core.windows.net/exercise01assets/accountkey.png)\n\n3. Paste the key, the account name, and some container name into the following cell and run the cell."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "ACCOUNT_NAME = '...'                   #see picture above\nACCOUNT_KEY  = '...'\nCONTAINER_NAME = 'exercise02'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n###  Step 2: Install a management library for Azure storage  and import it\n1. Run two cells below for that"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting azure-storage==0.33.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/d4/d26d82b468e1c2c7e9bf214dc75c4f0a6d10c4da839cc029e36662569c67/azure_storage-0.33.0-py3-none-any.whl (181kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 184kB 6.1MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: python-dateutil in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.33.0) (2.7.3)\nRequirement already satisfied: azure-common in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.33.0) (1.1.15)\nRequirement already satisfied: requests in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.33.0) (2.19.1)\nRequirement already satisfied: cryptography in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.33.0) (2.0.3)\nRequirement already satisfied: azure-nspkg in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.33.0) (2.0.0)\nRequirement already satisfied: six>=1.5 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from python-dateutil->azure-storage==0.33.0) (1.11.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from requests->azure-storage==0.33.0) (2018.8.24)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from requests->azure-storage==0.33.0) (3.0.4)\nRequirement already satisfied: urllib3<1.24,>=1.21.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from requests->azure-storage==0.33.0) (1.23)\nRequirement already satisfied: idna<2.8,>=2.5 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from requests->azure-storage==0.33.0) (2.7)\nRequirement already satisfied: asn1crypto>=0.21.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from cryptography->azure-storage==0.33.0) (0.24.0)\nRequirement already satisfied: cffi>=1.7 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from cryptography->azure-storage==0.33.0) (1.7.0)\nRequirement already satisfied: pycparser in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from cffi>=1.7->cryptography->azure-storage==0.33.0) (2.14)\nInstalling collected packages: azure-storage\nSuccessfully installed azure-storage-0.33.0\n"
                }
            ],
            "source": "!pip install azure-storage==0.33.0"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from azure.storage.blob import BlockBlobService\nfrom azure.storage.blob import PageBlobService\nfrom azure.storage.blob import AppendBlobService\nfrom azure.storage.blob import PublicAccess\nfrom azure.storage.models import LocationMode\nfrom azure.storage.blob import ContentSettings\nfrom azure.storage.blob import BlockListType\nfrom azure.storage.blob import BlobBlock\nfrom timeit import default_timer as timer\nimport uuid\nimport random\n\n#function for genereting unique names for blobs\ndef get_blob_name():\n    return '{}{}'.format('blob', str(uuid.uuid4()).replace('-', ''))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exercise 3"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Step 1: Explore Concepts of Azure Blob Storage"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "1. A container provides a grouping of a set of blobs. All blobs must be in a container. An account can contain an unlimited number of containers. A container can store an unlimited number of blobs. Note that the container name must be lowercase.\n![Image of blob](https://docs.microsoft.com/en-us/azure/includes/media/storage-blob-concepts-include/blob1.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "&nbsp;&nbsp;&nbsp; 2\\. We now want to understand the difference between blobs of distinct types.  <br>\n&nbsp;&nbsp;&nbsp; Read the following [link](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs) which contains a brief explanation regarding the different blob types, and then select (by putting an $X$ on the table below) which blob type is the most suitable for a particular use case."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "|                | Block Blob | Append Blob  | Page Blob |\n| --------------:| -----------| ------------:| ---------:|\n| Static content delivery             |            |              |           |\n| As a disk for a VirtualMachine       |            |              |           |\n| Streaming video                      |            |              |           |\n| Log Files                     |            |             |          |\n| Social network events (e.g., uploading photos to Instagram)          |            |              |           |"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n\n|                | Block Blob | Append Blob  | Page Blob |\n| --------------:| -----------| ------------:| ---------:|\n| Static content delivery              |     X      |             |           |\n| As a disk for a VirtualMachine       |            |             |     X     |\n| Streaming video                      |     X      |             |           |\n| Log Files                     |            |       X      |           |\n| Social network events (e.g., uploading photos to Instagram) |     X      |              |           |"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Block blobs__ let you upload large blobs efficiently. Block blobs are comprised of blocks, each of which is identified by a block ID. You create or modify a block blob by writing a set of blocks and committing them by their block IDs. Each block can be a different size, up to a maximum of 100 MB, and a block blob can include up to 50,000 blocks. The maximum size of a block blob is therefore slightly more than 4.75 TB (100 MB X 50,000 blocks). \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "An __append blob__ is comprised of blocks and is optimized for append operations. When you modify an append blob, blocks are added to the end of the blob only, via the append_block operation. Updating or deleting of existing blocks is not supported. Unlike a block blob, an append blob does not expose its block IDs.\n\nEach block in an append blob can be a different size, up to a maximum of 4 MB, and an append blob can include up to 50,000 blocks. The maximum size of an append blob is therefore slightly more than 195 GB (4 MB X 50,000 blocks)."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Page blobs__ are a collection of 512-byte pages optimized for random read and write operations. To create a page blob, you initialize the page blob and specify the maximum size the page blob will grow. To add or update the contents of a page blob, you write a page or pages by specifying an offset and a range that align to 512-byte page boundaries. A write to a page blob can overwrite just one page, some pages, or up to 4 MB of the page blob. Writes to page blobs happen in-place and are immediately committed to the blob. The maximum size for a page blob is 8 TB."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Step 2: Test your first container"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n\n&nbsp;&nbsp;&nbsp; 1\\. Now we need to create a new container under the specified account. If the container with the same name already exists, the operation fails and returns False"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "exercise02: Container already exists\n"
                }
            ],
            "source": "block_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n\nstatus = block_blob_service.create_container(CONTAINER_NAME)\n#status = block_blob_service.create_container(CONTAINER_NAME, \n#         public_access=PublicAccess.Container) #create the server with public access\nif status==True:\n\tprint(\"{0}: Container created!\".format(CONTAINER_NAME))\nelse:\n\tprint(\"{0}: Container already exists\".format(CONTAINER_NAME))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "&nbsp;&nbsp;&nbsp; 2\\. Download a file to your Jupyter's virtual machine."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2018-09-26 16:10:28--  https://www.vetbabble.com/wp-content/uploads/2016/11/hiding-cat.jpg\nResolving webproxy (webproxy)... 10.36.60.1\nConnecting to webproxy (webproxy)|10.36.60.1|:3128... connected.\nProxy request sent, awaiting response... 200 OK\nLength: 136356 (133K) [image/jpeg]\nSaving to: \u2018cat.jpg\u2019\n\ncat.jpg             100%[===================>] 133.16K  --.-KB/s    in 0.03s   \n\n2018-09-26 16:10:28 (4.22 MB/s) - \u2018cat.jpg\u2019 saved [136356/136356]\n\n"
                }
            ],
            "source": "!wget https://www.vetbabble.com/wp-content/uploads/2016/11/hiding-cat.jpg -O cat.jpg"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "&nbsp;&nbsp;&nbsp; 3\\. Finally, upload the file to the blob storage\n\nNote: The name of the file on local machine can differ from the name of the blob\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "https://ex02ethbg2018.blob.core.windows.net/exercise02/picture\n"
                }
            ],
            "source": "local_file = \"cat.jpg\"\n\nblob_name = \"picture\"\ntry:\n    block_blob_service.create_blob_from_path(\n        CONTAINER_NAME,\n        blob_name,\n        local_file,\n        content_settings=ContentSettings(content_type='image/jpg')\n    )\n    \n    print(block_blob_service.make_blob_url(CONTAINER_NAME,blob_name))\nexcept:\n    print (\"something wrong happened when uploading the data %s\"%blob_name)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "&nbsp;&nbsp;&nbsp; 4\\. Try to open the link above\n\nBy default, the new container is private, so you must specify your storage access key (as you did earlier) to download blobs from this container. If you want to make the blobs within the container available to everyone, you can create the container and pass the public access level using the following code.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<azure.storage.blob.models.ResourceProperties at 0x7f97513ac5c0>"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "block_blob_service.set_container_acl(CONTAINER_NAME, public_access=PublicAccess.Container)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "After this change, anyone on the Internet can see blobs in a public container, but only you can modify or delete them. <br>\nNow, try to open the link again (It takes few seconds to change access permisions)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "&nbsp;&nbsp;&nbsp; 5\\. List all blobs in the container"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "To list the blobs in a container, use the list_blobs method. This method returns a generator. The following code outputs name, type, size and url of each blob in a container"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "scrolled": true,
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Name: picture \t\t Type: BlockBlob \t\t Size: 136356\nhttps://ex02ethbg2018.blob.core.windows.net/exercise02/picture\n"
                }
            ],
            "source": "# List all blobs in the container\nblobs = block_blob_service.list_blobs(CONTAINER_NAME)\nfor blob in blobs:\n    try:\n        print('Name: {} \\t\\t Type: {} \\t\\t Size: {}'.format(blob.name,blob.properties.blob_type, blob.properties.content_length)) \n        print(block_blob_service.make_blob_url(CONTAINER_NAME,blob.name)) #We can also print the url of a blob\n    except:\n        print(\"something went wrong\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "&nbsp;&nbsp;&nbsp; 8\\. Download blobs\n\nTo download data from a blob, use get_blob_to_path, get_blob_to_stream, get_blob_to_bytes, or get_blob_to_text. They are high-level methods that perform the necessary chunking when the size of the data exceeds 64 MB.\n\nNote: The Name of a file after downloading can differ from the name of a blob\n\nThe following example demonstrates using get_blob_to_path to download the content of your container and store it with names file_i where i is a sequential index."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "scrolled": true,
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "success for ./file_0\n"
                }
            ],
            "source": "LOCAL_DIRECT = \"./\"   \n\nblobs = block_blob_service.list_blobs(CONTAINER_NAME)\ni=0\nfor blob in blobs:\n    local_file = LOCAL_DIRECT + 'file_{}'.format(str(i))\n    i=i+1\n    try:\n        block_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n        print(\"success for %s\"%local_file)\n    except:\n        print(\"something went wrong with the blob: %s\"%blob.name)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Step 3: Using the REST API"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "REpresentational State Transfer (__REST__), or __RESTful__, web services provide interoperability between computer systems on the Internet. REST-compliant web services allow the requesting systems to access and manipulate textual representations of web resources by using a uniform and predefined set of *stateless* operations.\n\nThe most popular operations in REST are GET, POST, PUT, DELETE. A response may be in XML, HTML, JSON, or some other format. <br>\nYou can find the Azure Storage Service HTTP response codes in the following [link](https://docs.microsoft.com/en-us/rest/api/storageservices/status-and-error-codes2), and the HTTP response codes defined by the World Wide Web Consortium (W3C) [here](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We could use tools like [Postman](https://www.getpostman.com/), CURL or others to make REST requests to Azure Storage.\n<br>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Task 1.\n1. Use any tool for listing all blobs in your container. \n<br> For this use the following [REST request](https://docs.microsoft.com/en-us/rest/api/storageservices/list-blobs). \n<br> To avoid setting up an authentication you can make your container public by changing access policy to **\"Container\"**. See Pictures below:\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/container.png)\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/policy.png)\n\n2. Explain why the request above does not include **body** part.\n3. What is the response format of the request? "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__ Using Postman.\n\nYou can use as URI the following, but replacing the storage account name with what you chose instead of $myaccount$ and the container we set up in this exercise instead of $mycontainer$. <br>\n\nhttps://myaccount.blob.core.windows.net/mycontainer?restype=container&comp=list\n\nAfter performing the request successfully, you should see something similar to the image below.\n\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/getRequestResult.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution2:__ Using curl."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\ufeff<?xml version=\"1.0\" encoding=\"utf-8\"?><EnumerationResults ContainerName=\"https://ex02ethbg2018.blob.core.windows.net/exercise02\"><Blobs><Blob><Name>picture</Name><Url>https://ex02ethbg2018.blob.core.windows.net/exercise02/picture</Url><Properties><Last-Modified>Wed, 26 Sep 2018 16:10:34 GMT</Last-Modified><Etag>0x8D623CA975341E1</Etag><Content-Length>136356</Content-Length><Content-Type>image/jpg</Content-Type><Content-Encoding /><Content-Language /><Content-MD5>8ZGxCm3vBjOkcN4SbNK2IQ==</Content-MD5><Cache-Control /><BlobType>BlockBlob</BlobType><LeaseStatus>unlocked</LeaseStatus></Properties></Blob></Blobs><NextMarker /></EnumerationResults>"
                }
            ],
            "source": "!curl \"https://ex02ethbg2018.blob.core.windows.net/exercise02?restype=container&comp=list\""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Exercise 4. KeyValue Vector Clocks\n\nAs pointed out in the lecture already, the concepts are clearly explained in the Dynamo paper by the DeCandia, G., et. al. (2007). \"Dynamo: Amazon\u2019s Highly Available Key-value Store\". In SOSP \u201907 (Vol. 41, p. 205). [DOI](https://dl.acm.org/citation.cfm?doid=1294261.1294281)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Task 1\nMultiple distributed hash tables use vector clocks for capturing causality among different versions of the same object. In Amazon's Dynamo, a vector clock is associated with every version of every object.\n\nLet $VC$ be an $N$-element array which contains non-negative integers, initialized to 0, representing $N$ logical clocks of the $N$ processes (nodes) of the system. $VC$ gets its $j$ element incremented by one everytime node $j$ performs a write operation on it. <br>\nMoreover, $VC(x)$ denotes the vector clock of a write event, and $VC(x)_z$ denotes the element of that clock for the node $z$.\n\nTry to __formally define__ the partial ordering that we get from using vector clocks.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution__:\n\n$VC(x) \\leq VC(y) \\iff \\forall z[VC(x)_z \\leq VC(y)_z]$"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Task 2\n\nVector clock antisymmetry property is defined as follows:\n\nif $ VC(x) \\lt VC(y)$, then $ \u00ac \\ (VC(y) \\lt VC(x)) $\n\nProof such property."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n\nFor $ VC(x) \\lt VC(y)$ to be true, then $VC(x)$ has to be strictly lower than $VC(y)$. Thus, the partial order definition can be extended as follows:\n\n$VC(x) \\lt VC(y) \\iff \\forall z[VC(x)_z \\leq VC(y)_z] \\wedge \\exists z'[VC(x)_{z'} \\lt VC(y)_{z'}]$\n\nWe can start with the hypothesis that $VC(x) \\lt VC(y)$ is true. This means that there is a z' element for which $VC(x) \\lt VC(y)$. <br>\nThen to proof by contradiction we assume, that such $z'$ element does not exist. <br>\nThis cannot be case as to we require a strict partial order of $VC(x)$ and $VC(y)$ as part of our hypothesis."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Task 3\n\nConsider $j$ servers in a cluster where $S_j$ denotes the $j$th node. Also, for this exercise a vector clock is specified using an array of integer numbers where the element at the $j$ position represents a write from the $S_j$ node. <br>\nSo, given the following version evolution DAG for a particular object, complete the vector clocks computed at the corresponding version.\n\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/vc2.png)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n    &nbsp; <br>\n    \n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/vc2solution.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Task 4\n\nWhen a __GET request__ comes in to Amazon Dynamo with some key, then:\n  - The coordinator node (selected from the preference list as the top node for this key) is taking care of this request\n  - The coordinator node requests from other nodes (itself + the next N-1 healthy ones on the preference list), and receives, a whole bunch of versions for the value associated with the key, that are modelled as (value, vector clock) pairs such as (a, [1, 3, 2])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Task 4.1\nGiven the following list of versions, draw the version DAG that the coordinator node will build for returning available versions.\n\n(1, [0,0,1])  <br>(1, [0,1,1])<br>(2, [1,1,1])<br>(3, [0,2,1])<br>(10, [1,3,1])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n&nbsp;\nWe could use the following rules as guidance for creating correct version DAGs. <br>\n\n- If no partial ordering exists between nodes, then those nodes should be on the same level, i.e., they are both valid versions.\n- If there is an edge between two nodes, then the \u201cfather node\u201d should be smaller than the child.\n- You cannot have skip connections, i.e., there cannot be an edge from a grandfather node directly to a child node.\n- Transitive edges shouldn't be present in the version DAG.\n- Each edge represents the update of exactly one entry of the vector clock.\n\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/vc31.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Task 4.2\nGiven the following list of versions, draw the version DAG that the coordinator node will build for returning the correct version.\n\n\n (a, [1,0,0]) (b, [0,1,0])<br>\n (c, [2,1,0]) <br>\n (d, [2,1,1]) <br>\n (e, [3,1,1]) (f, [2,2,1]) <br>\n (g, [3,1,2]) <br>\n (h, [3,2,3]) (i, [4,2,2]) <br>\n (j, [5,2,2])<br>\n (k, [4,3,3]) (l, [5,2,3])<br>\n (m, [5,4,3]) (n, [6,3,3])<br>\n (o, [6,4,4])<br>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n    &nbsp;\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/vc32.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Task 4.3\nGiven the following list of versions, draw the version DAG that the coordinator node will build for returning the correct version.\n\n(a, [1,0,0,0])<br>\n(b, [0,0,0,1])<br>\n(aa, [0,0,1,0])<br>\n(bb, [0,1,0,0])<br>\n(c, [1,2,0,1])<br>\n(cc, [0,1,1,2])<br>\n(d, [1,3,0,1])<br>\n(f, [1,2,0,2])<br>\n(e, [2,1,1,2])<br>\n(g, [2,2,2,2])<br>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n\nWith the given input it is actually not possible to create a correct version DAG. <br>\nThe following sequence of events would create a correct version DAG. <br>\n\n(a, [1,0,0,0])<br>\n(b, [0,0,0,1])<br>\n(aa, [0,0,1,0])<br>\n(bb, [0,1,0,0])<br>\n(c, [1,2,0,1])<br>\n(cc, [0,1,1,2])<br>\n(d, [1,3,0,1])<br>\n(f, [1,2,1,3])<br>\n(e, [2,1,1,2])<br>\n(g, [2,2,2,3])<br> <br>\nBy using this input, a correct version DAG would be the following:<br>\n\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/vc331.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Task 5\n\nConsider $j$ servers in a cluster where $S_j$ denotes the $j$th node. The following table denotes the execution of a series of get/put operations. Also each line of the table represents the events that happen at the time time. For example, at time 0 (t0) servers $S_1$ and $S_3$ perform operations. <br>\nComplete the values and vector clocks of each get operation contex where each context will contain (list_values,vector_clock).\n\n<table>\n  <tr><th></th><th>S0</th><th>S1</th><th>S2</th><th>S3</th><th>S4</th></tr>\n  <tr>\n    <td>t0</td>\n    <td></td>\n    <td>Get(1)$\\rightarrow$ _______________,$Cxt_1$(_______________)<br>Put(1,$Cxt_1$, \u201da\u201d)</td>\n    <td></td>\n    <td>Get(1)$\\rightarrow$ _______________,$Cxt_2$(_______________)<br>Put(1,$Cxt_2$, \u201dbb\u201d)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>t2</td>\n    <td>Get(1)$\\rightarrow$ _______________,$Cxt_4$(_______________)<br>Put (1, $Cxt_4$, \u201crr\u201d)</td>\n    <td>Get(1)$\\rightarrow$ _______________,$Cxt_5$(_______________)<br>Put (1, $Cxt_5$, \u201ddd\u201d)</td>\n    <td></td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>t4</td>\n    <td></td>\n    <td></td>\n    <td>Get(1)$\\rightarrow$ _______________,$Cxt_9$(_______________) <br>Put(1, $Cxt_9$, \u201dccc\u201d)</td>\n    <td>Get(1)$\\rightarrow$ _______________,$Cxt_{10}$(_______________) <br> Put(1, $Cxt_{10}$, \u201ddd\u201d)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>t5</td>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td>Get(1)$\\rightarrow$ _______________,$Cxt_{11}$(_______________) <br>Put(1, $Cxt_{11}$, \u201cfff\u201d)</td>\n  </tr>\n</table>\n\nThe DAG below shows the interaction among nodes when retrieving values. You can use it for determining the expected values.\n\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/vc4_solution.png)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n<table>\n  <tr><th></th><th>S0</th><th>S1</th><th>S2</th><th>S3</th><th>S4</th></tr>\n  <tr>\n    <td>t0</td>\n    <td></td>\n    <td>Get(1)$\\rightarrow Cxt_1$()<br>Put(1,$Cxt_1$, \u201da\u201d)</td>\n    <td></td>\n    <td>Get(1)$\\rightarrow Cxt_2$()<br>Put(1,$Cxt_2$, \u201dbb\u201d)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>t2</td>\n    <td>Get(1)$\\rightarrow [a, bb],Cxt_4$([0,1,0,1,0])<br>Put (1, $Cxt_4$, \u201crr\u201d)</td>\n    <td>Get(1)$\\rightarrow [a, bb],Cxt_5$([0,1,0,1,0])<br>Put (1, $Cxt_5$, \u201ddd\u201d)</td>\n    <td></td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>t4</td>\n    <td></td>\n    <td></td>\n    <td>Get(1)$\\rightarrow [rr],Cxt_9$([1,1,0,1,0]) <br>Put(1, $Cxt_9$, \u201dccc\u201d)</td>\n    <td>Get(1)$\\rightarrow [dd],Cxt_{10}$([0,2,0,1,0]) <br> Put(1, $Cxt_{10}$, \u201ddd\u201d)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>t5</td>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td>Get(1)$\\rightarrow [ccc, dd],Cxt_{11}$([1,2,1,2,0]) <br>Put(1, $Cxt_{11}$, \u201cfff\u201d)</td>\n  </tr>\n</table>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Exercise 5. Merkle Trees\nA hash tree or Merkle tree is a binary tree in which every leaf node gets as its label a data block and every non-leaf node is labelled with the cryptographic hash of the labels of its child nodes. \nSome KeyValue stores use Merkle trees for __detecting inconsistencies in data between replicas__ because differences in data blocks stored can be efficiently found and then this can help in reducing the amount of data that needs to be transferred to compare data blocks. <br>\nThis works by exchaging first the root hash, comparing it with their own, and if this does not match the one the replica contains, then the children of the node will be retrieved, and their hashes compared. If the hash of a node matches, then its children are not fetched because their are in sync. However if they are not, then its children are fetched to be compared.\n\n## Task 1\nThe picture below depicts two Merkle trees each of one belonging to two different replicas and where each tree node and leaf are labeled with a different letter. However, these replicas have run out of sync. <br>\nSpecify the tree nodes that will have to be exchanged in order to synchronize both trees.\n\n\n![](https://ethbigdata2017.blob.core.windows.net/exercise02assets/mktree1.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n- Node A (root hash) is exchanged. Node A is not the same.\n- Fetch Node A children. \n- Nodes B and C are exchanged. Node B is the same so its children are not exchanged, but node C is not.\n- Fetch Node C children.\n- Nodes F and G are exchanged. Node F is the same so its children are not exchanged, but node G is not.\n- Fetch Node G children.\n- Nodes N and O are exchanged. They both differ, so data blocks represented by the leaf's hashes."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Exercise 6. Virtual nodes\n\nVirtual nodes were introduced to avoid assigning data in an unbalanced manner and coping with hardware heterogeneity by taking into consideration the physical capacity of each server\n\nLet assume we have ten servers ($i_1$ to $i_{10}$) each with the following amount of main memory: 8GB, 16GB, 32GB, 8GB, 16GB, 0.5TB, 1TB, 0.25TB, 10GB, 20GB. Calculate the number of virtual nodes/tokens each server should get according to its main memory capacity if we want to have a total of 256 virtual nodes/tokens."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "__Solution:__\n\nThe total amount of main memory available is: 1902GB <br>\nSo if we want to have 256 virtual nodes/tokens, each should get ~7.42GB. <br>\nThus, each physical server $i_1$ to $i_{10}$ should get around: 1, 2, 4, 1, 2, 69, 138, 34, 1, 3 number of virtual nodes/tokens.\n<br> <br>\nIn general, rounding up to the next integer number or rounding down to the previous integer number is not determinant as virtual nodes are logically positioned in random parts of the ring anyway. <br>\nHowever, one thing to be taken into consideration in practice is that the server with the smallest capacity should have more than a single virtual node. This is because it might be \"unlucky\" and have to handle a large domain of responsability."
        }
    ],
    "metadata": {
        "anaconda-cloud": {},
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.5.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
